{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!** This notebook has to be run after the following notebooks:\n",
    "- [dataset.ipynb](dataset.ipynb)\n",
    "- [scoring.ipynb](scoring.ipynb)\n",
    "- [model.ipynb](model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import dataset\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using constant padding\n",
      "| using scales: [0.8333333333333334, 1.0, 1.2]\n",
      "| using ordinary correlation\n",
      "load pretrained model from models/SiamSE/checkpoint_vot.pth\n",
      "remove prefix \"module.\"\n",
      "missing keys:set()\n",
      "unused checkpoint keys:set()\n"
     ]
    }
   ],
   "source": [
    "models = model.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading dataset David gt and image files have different length\n",
      "Error while loading dataset Football1 gt and image files have different length\n",
      "Error while loading dataset Jogging1 gt and image files have different length\n",
      "Error while loading dataset Jogging2 gt and image files have different length\n",
      "Error while loading dataset Subway gt and image files have different length\n"
     ]
    }
   ],
   "source": [
    "datasets = dataset.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['SEsiamFC']), dict_keys(['mytc128', 'myvot2021']))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys(), datasets.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = models['SEsiamFC']\n",
    "d1 = datasets['mytc128']\n",
    "v1 = d1['Cup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_video(tracker, video, verbose=0, save_video=None):\n",
    "    start_frame, toc = 0, 0\n",
    "\n",
    "    pred = []\n",
    "    image_files, gt = video['image_files'], video['gt']\n",
    "\n",
    "    if save_video is not None:\n",
    "        sh = cv2.imread(image_files[0]).shape[:2]\n",
    "        frame_size = (sh[1], sh[0])\n",
    "        out = cv2.VideoWriter(save_video, cv2.VideoWriter_fourcc(*'DIVX'), 20.0, frame_size)\n",
    "\n",
    "\n",
    "    for f, image_file in enumerate(image_files):\n",
    "        tic = cv2.getTickCount()\n",
    "\n",
    "        if f == start_frame:  # init\n",
    "            tracker.initialize(image_file, np.array(gt[f]))\n",
    "            pred.append(gt[f])\n",
    "            \n",
    "            if save_video is not None:\n",
    "                im = cv2.imread(image_file)\n",
    "                cv2.rectangle(im, (int(gt[f][0]), int(gt[f][1])), (int(gt[f][0] + gt[f][2]), int(gt[f][1] + gt[f][3])), (0, 255, 0), 2)\n",
    "                out.write(im)\n",
    "\n",
    "        elif f > start_frame:  # tracking\n",
    "            pred_bbox = tracker.track(image_file)\n",
    "            b_overlap = scoring.get_precision(gt[f], pred_bbox)\n",
    "            if b_overlap > 0:\n",
    "                pred.append(pred_bbox)\n",
    "            else:\n",
    "                pred.append(2)\n",
    "                start_frame = f + 5\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(\"{} gt: {} pred: {} overlap: {}\".format(f, gt[f], pred_bbox, b_overlap))\n",
    "\n",
    "            if save_video is not None:\n",
    "                im = cv2.imread(image_file)\n",
    "                cv2.rectangle(im, (int(gt[f][0]), int(gt[f][1])), (int(gt[f][0] + gt[f][2]), int(gt[f][1] + gt[f][3])), (0, 255, 0), 2)\n",
    "                cv2.rectangle(im, (int(pred_bbox[0]), int(pred_bbox[1])), (int(pred_bbox[0] + pred_bbox[2]), int(pred_bbox[1] + pred_bbox[3])), (0, 0, 255), 2)\n",
    "                out.write(im)\n",
    "\n",
    "        toc += cv2.getTickCount() - tic\n",
    "\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    precisions = [scoring.get_precision(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    precisions = np.array(precisions)\n",
    "    mprec = np.mean(precisions)\n",
    "\n",
    "    success = [scoring.is_success(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    success = np.array(success)\n",
    "    msucc = np.mean(success)\n",
    "\n",
    "    fps = f / toc\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Video: {:12s} Time: {:2.1f}s Speed: {:5.2f}fps mSuccess: {:3.2f} mPrecision {:3.2f}'.format(video['name'], toc, f / toc, msucc, mprec))\n",
    "    \n",
    "    if save_video is not None:\n",
    "        out.release()\n",
    "\n",
    "    return mprec, msucc, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: Cup          Time: 40.9s Speed:  7.39fps mSuccess: 1.00 mPrecision 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8127891293539966, 1.0, 7.3858280901800315)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_video(t1, v1, verbose=1, save_video='SEsiamFC_mytc128_Cup.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_dataset(tracker, dataset, dataset_name, n = 1, verbose=0, save_results=False):\n",
    "    precisions = []\n",
    "    success = []\n",
    "    fpss = []\n",
    "    k = 0\n",
    "\n",
    "    if save_results:\n",
    "        if not os.path.exists(os.path.join(\"results\")):\n",
    "            os.makedirs(os.path.join(\"results\"))\n",
    "\n",
    "        if not os.path.exists(os.path.join(\"results\", tracker.model_name)):\n",
    "            os.makedirs(os.path.join(\"results\", tracker.model_name))\n",
    "\n",
    "    j = {}\n",
    "\n",
    "    for video in dataset:\n",
    "        if k == n:\n",
    "            break\n",
    "\n",
    "        precision, succes, fps = track_video(tracker, dataset[video], verbose)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        success.append(succes)\n",
    "        fpss.append(fps)\n",
    "\n",
    "        if save_results:\n",
    "            j[video] = {'precision': precision, 'succes': succes, 'fps': fps}\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    if save_results:\n",
    "        with open(os.path.join(\"results\", tracker.model_name, dataset_name + '.json'), 'w') as f:\n",
    "            json.dump(j, f, indent=2)\n",
    "\n",
    "    return np.mean(precisions), np.mean(success), np.mean(fpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: Airport_ce   Time: 20.6s Speed:  7.14fps mSuccess: 0.91 mPrecision 0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7812066443927109, 0.9054054054054054, 7.135532508335685)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_dataset(t1, d1, 'mytc128', verbose=1, save_results=True)\n",
    "track_dataset(t1, d1, 'mytc128', verbose=1, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_benchmark(models, datasets, verbose=0, save_results=False):\n",
    "    for model_name in models:\n",
    "        for dataset_name in datasets:\n",
    "            print(\"Benchmarking {} on {}\".format(model_name, dataset_name))\n",
    "            track_dataset(models[model_name], datasets[dataset_name], dataset_name, verbose, save_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96d6dd9d829a4a8dd1edda94add76f4a08841e645b8132fdf668d21ba72dd652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
