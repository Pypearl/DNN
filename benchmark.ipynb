{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!** This notebook has to be run after the following notebooks:\n",
    "- [dataset.ipynb](dataset.ipynb)\n",
    "- [scoring.ipynb](scoring.ipynb)\n",
    "- [model.ipynb](model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import dataset\n",
    "import scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| using constant padding\n",
      "| using scales: [0.8333333333333334, 1.0, 1.2]\n",
      "| using ordinary correlation\n",
      "load pretrained model from models/SiamSE/checkpoint_vot.pth\n",
      "remove prefix \"module.\"\n",
      "missing keys:set()\n",
      "unused checkpoint keys:set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/33652/Documents/ING 2 Prog/DeepNN/venv_dnn/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/33652/Documents/ING 2 Prog/DeepNN/venv_dnn/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "models = model.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading dataset David gt and image files have different length\n",
      "Error while loading dataset Football1 gt and image files have different length\n",
      "Error while loading dataset Jogging1 gt and image files have different length\n",
      "Error while loading dataset Jogging2 gt and image files have different length\n",
      "Error while loading dataset Subway gt and image files have different length\n"
     ]
    }
   ],
   "source": [
    "datasets = dataset.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['SEsiamFC', 'AAA']), dict_keys(['mytc128', 'myvot2021']))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys(), datasets.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = models['SEsiamFC']\n",
    "d1 = datasets['mytc128']\n",
    "v1 = d1['Cup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(out, image_file, gt, pred=None):\n",
    "    im = cv2.imread(image_file)\n",
    "    cv2.rectangle(im, (int(gt[0]), int(gt[1])), (int(gt[0] + gt[2]), int(gt[1] + gt[3])), (0, 255, 0), 2)\n",
    "    if pred is not None:\n",
    "        cv2.rectangle(im, (int(pred[0]), int(pred[1])), (int(pred[0] + pred[2]), int(pred[1] + pred[3])), (0, 0, 255), 2)\n",
    "    out.write(im)\n",
    "\n",
    "def track_video(tracker, video, verbose=0, save_video=None):\n",
    "    start_frame, toc = 0, 0\n",
    "\n",
    "    pred = []\n",
    "    image_files, gt = video['image_files'], video['gt']\n",
    "\n",
    "    if save_video is not None:\n",
    "        sh = cv2.imread(image_files[0]).shape[:2]\n",
    "        frame_size = (sh[1], sh[0])\n",
    "        out = cv2.VideoWriter(save_video, cv2.VideoWriter_fourcc(*'DIVX'), 20.0, frame_size)\n",
    "\n",
    "\n",
    "    for f, image_file in enumerate(image_files):\n",
    "        tic = cv2.getTickCount()\n",
    "\n",
    "        if f == start_frame:  # init\n",
    "            tracker.initialize(image_file, np.array(gt[f]))\n",
    "            pred.append(gt[f])\n",
    "            \n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f])\n",
    "\n",
    "        elif f > start_frame:  # tracking\n",
    "            pred_bbox = tracker.track(image_file)\n",
    "            b_overlap = scoring.get_precision(gt[f], pred_bbox)\n",
    "            if b_overlap > 0:\n",
    "                pred.append(pred_bbox)\n",
    "            else:\n",
    "                pred.append(2)\n",
    "                start_frame = f + 5\n",
    "\n",
    "            if verbose > 1:\n",
    "                print(\"{} gt: {} pred: {} overlap: {}\".format(f, gt[f], pred_bbox, b_overlap))\n",
    "\n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f], pred_bbox)\n",
    "\n",
    "        else:\n",
    "            pred.append(0)\n",
    "            if save_video is not None:\n",
    "                write_video(out, image_file, gt[f])\n",
    "\n",
    "        toc += cv2.getTickCount() - tic\n",
    "\n",
    "    toc /= cv2.getTickFrequency()\n",
    "\n",
    "    precisions = [scoring.get_precision(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    precisions = np.array(precisions)\n",
    "    mprec = np.mean(precisions)\n",
    "\n",
    "    success = [scoring.is_success(gt[i], pred[i]) for i in range(len(gt))]\n",
    "    success = np.array(success)\n",
    "    msucc = np.mean(success)\n",
    "\n",
    "    fps = f / toc\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('Video: {:12s} Time: {:2.1f}s Speed: {:5.2f}fps mSuccess: {:3.2f} mPrecision {:3.2f}'.format(video['name'], toc, f / toc, msucc, mprec))\n",
    "    \n",
    "    if save_video is not None:\n",
    "        out.release()\n",
    "\n",
    "    return mprec, msucc, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_video(t1, v1, verbose=1, save_video='SEsiamFC_mytc128_Cup.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_dataset(tracker, dataset, dataset_name, n = 3, verbose=0, save_results=False):\n",
    "    precisions = []\n",
    "    success = []\n",
    "    fpss = []\n",
    "    k = 0\n",
    "\n",
    "    if save_results:\n",
    "        if not os.path.exists(os.path.join(\"results\")):\n",
    "            os.makedirs(os.path.join(\"results\"))\n",
    "\n",
    "        if not os.path.exists(os.path.join(\"results\", tracker.model_name)):\n",
    "            os.makedirs(os.path.join(\"results\", tracker.model_name))\n",
    "\n",
    "    j = {}\n",
    "\n",
    "    for video in dataset:\n",
    "        if k == n:\n",
    "            break\n",
    "\n",
    "        precision, succes, fps = track_video(tracker, dataset[video], verbose)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        success.append(succes)\n",
    "        fpss.append(fps)\n",
    "\n",
    "        if save_results:\n",
    "            j[video] = {'precision': precision, 'succes': succes, 'fps': fps}\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    if save_results:\n",
    "        with open(os.path.join(\"results\", tracker.model_name, dataset_name + '.json'), 'w') as f:\n",
    "            json.dump(j, f, indent=2)\n",
    "\n",
    "    return np.mean(precisions), np.mean(success), np.mean(fpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_dataset(t1, d1, 'mytc128', n = 3, verbose=1, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_benchmark(models, datasets, verbose=0, save_results=False):\n",
    "    for model_name in models:\n",
    "        for dataset_name in datasets:\n",
    "            print(\"Benchmarking {} on {}\".format(model_name, dataset_name))\n",
    "            track_dataset(models[model_name], datasets[dataset_name], dataset_name, verbose=verbose, save_results=save_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking SEsiamFC on mytc128\n",
      "Video: Airport_ce   Time: 21.4s Speed:  6.88fps mSuccess: 0.91 mPrecision 0.78\n",
      "Video: Baby_ce      Time: 49.1s Speed:  6.00fps mSuccess: 1.00 mPrecision 0.64\n",
      "Video: Badminton_ce1 Time: 95.1s Speed:  6.08fps mSuccess: 1.00 mPrecision 0.74\n",
      "3 3\n",
      "Benchmarking SEsiamFC on myvot2021\n",
      "Video: agility      Time: 14.8s Speed:  6.67fps mSuccess: 0.66 mPrecision 0.45\n",
      "Video: animal       Time: 13.1s Speed:  7.55fps mSuccess: 0.65 mPrecision 0.49\n",
      "Video: ants1        Time: 70.3s Speed:  4.61fps mSuccess: 0.90 mPrecision 0.50\n",
      "3 3\n",
      "Benchmarking AAA on mytc128\n",
      "Video: Airport_ce   Time: 1.6s Speed: 92.61fps mSuccess: 0.17 mPrecision 0.17\n",
      "Video: Baby_ce      Time: 2.7s Speed: 110.60fps mSuccess: 0.17 mPrecision 0.17\n",
      "Video: Badminton_ce1 Time: 5.7s Speed: 101.95fps mSuccess: 0.17 mPrecision 0.17\n",
      "3 3\n",
      "Benchmarking AAA on myvot2021\n",
      "Video: agility      Time: 1.1s Speed: 91.22fps mSuccess: 0.17 mPrecision 0.17\n",
      "Video: animal       Time: 1.2s Speed: 83.05fps mSuccess: 0.17 mPrecision 0.17\n",
      "Video: ants1        Time: 3.7s Speed: 86.84fps mSuccess: 0.17 mPrecision 0.17\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "do_benchmark(models, datasets, verbose=1, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96d6dd9d829a4a8dd1edda94add76f4a08841e645b8132fdf668d21ba72dd652"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
