{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the following dataset to evaluate different models on tracking tasks:\n",
    "\n",
    "-   [TC-128](https://www3.cs.stonybrook.edu/~hling/data/TColor-128/TColor-128.html#dataset) (Present in the article) (4.37 Go)\n",
    "-   [VOT2021](https://www.votchallenge.net/vot2021/dataset.html) (Recent dataset) (1.23 Go)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook display samples of datasets and how they are modified to feat the models.\n",
    "\n",
    "To feat the models evaluation. The folder should be in the **data** folder and be structured as follow:\n",
    "\n",
    "-   *my*dataset_name\n",
    "    -   _img_\n",
    "        -   img1.jpg\n",
    "        -   img2.jpg\n",
    "        -   ...\n",
    "    -   _groundtruth.txt_\n",
    "\n",
    "---\n",
    "\n",
    "groundtruth.txt should be structured as follow:\n",
    "\n",
    "```\n",
    "x,y,w,h\n",
    "x,y,w,h\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import shutil\n",
    "import urllib.request\n",
    "import vot\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from vot import dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the location with the following cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "path_to_dataset = Path.cwd() / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import path_to_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TC-128\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 128 videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tc128_and_format(all: bool = False):\n",
    "    \"\"\"Download the TColor-128 dataset and format it to fit the code.\n",
    "\n",
    "    Args:\n",
    "        all (bool, optional): Download all the sequences or only the first one. Defaults to False.\n",
    "    \"\"\"\n",
    "    data_folder: Path = path_to_dataset / \"mytc128\"\n",
    "    if data_folder.exists():\n",
    "        return\n",
    "\n",
    "    data_folder.mkdir(parents=True)\n",
    "\n",
    "    with urllib.request.urlopen(\n",
    "        \"https://www3.cs.stonybrook.edu/~hling/data/TColor-128/seqs/\"\n",
    "    ) as f:\n",
    "        html: str = f.read().decode(\"utf-8\")\n",
    "        zip: List[str] = re.findall(r'href=\"(.*?\\.zip)\"', html)\n",
    "\n",
    "    for i in range(len(zip)):\n",
    "        zip_without_ext: str = zip[i][:-4]\n",
    "        path_zip: Path = data_folder / zip[i]\n",
    "        path_zip_without_ext: Path = data_folder / zip_without_ext\n",
    "\n",
    "        # Download zip file\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://www3.cs.stonybrook.edu/~hling/data/TColor-128/seqs/\" + zip[i],\n",
    "            path_zip,\n",
    "        )\n",
    "        shutil.unpack_archive(path_zip, data_folder)\n",
    "        path_zip.unlink()\n",
    "\n",
    "        # Remove useless files\n",
    "        attribute_file: str = zip_without_ext + \"_att.txt\"\n",
    "        frame_file: str = (\n",
    "            zip_without_ext if zip_without_ext != \"Jogging2\" else \"jogging2\"\n",
    "        ) + \"_frames.txt\"  # Special case for Jogging2\n",
    "\n",
    "        (path_zip_without_ext / attribute_file).unlink()\n",
    "        (path_zip_without_ext / frame_file).unlink()\n",
    "\n",
    "        # Rename groundtruth file to fit the code\n",
    "        groundtruth_file: str = zip_without_ext + \"_gt.txt\"\n",
    "        (path_zip_without_ext / groundtruth_file).rename(\n",
    "            path_zip_without_ext / \"groundtruth.txt\"\n",
    "        )\n",
    "\n",
    "        if not all:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_tc128_and_format(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOT 2021\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains semantic labelisation of 60 videos. We will use the semantic labelisation to compute the ground truth bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic2bbox(path_semantic: str, path_bbox: str):\n",
    "    \"\"\"Convert semantic segmentation to bounding box\n",
    "\n",
    "    Args:\n",
    "        path_semantic (str): Path to the semantic segmentation file\n",
    "        path_bbox (str): Path to the bounding box file\n",
    "    \"\"\"\n",
    "    with open(path_semantic, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        semantic = list(reader)\n",
    "\n",
    "    semantic = list(map(lambda x: [x[0][1:]] + x[1:], semantic))\n",
    "    semantic = list(map(lambda x: list(map(lambda y: int(y), x)), semantic))\n",
    "    semantic = list(map(lambda x: x[:4], semantic))\n",
    "\n",
    "    with open(path_bbox, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_vot2021_dataset_and_format(all: bool = False):\n",
    "    \"\"\"Download the VOT2021 dataset and format it to fit the code.\n",
    "\n",
    "    Args:\n",
    "        all (bool, optional): Download all the sequences or only the first one. Defaults to False.\n",
    "    \"\"\"\n",
    "    path_vot2021: Path = path_to_dataset / \"vot2021\"\n",
    "    path_myvot2021: Path = path_to_dataset / \"myvot2021\"\n",
    "\n",
    "    if path_myvot2021.exists():\n",
    "        return\n",
    "\n",
    "    dataset.download_dataset(dataset.vot._VOT_DATASETS[\"vot-st2021\"], path_vot2021)\n",
    "    path_myvot2021.mkdir(parents=True)\n",
    "\n",
    "    for video in path_vot2021.iterdir():\n",
    "        if not (path_vot2021 / video).is_dir():\n",
    "            continue\n",
    "\n",
    "        (path_myvot2021 / video).mkdir(exist_ok=True)\n",
    "\n",
    "        semantic2bbox(\n",
    "            path_vot2021 / video / \"groundtruth.txt\",\n",
    "            path_myvot2021 / video / \"groundtruth.txt\",\n",
    "        )\n",
    "        shutil.copytree(path_vot2021 / video / \"color\", path_myvot2021 / video / \"img\")\n",
    "\n",
    "        if not all:\n",
    "            break\n",
    "\n",
    "    shutil.rmtree(path_vot2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_vot2021_dataset_and_format(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a dataset.py\n",
    "\n",
    "def load_dataset(name: str) -> Dict:\n",
    "    \"\"\"Load a dataset.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the dataset to load.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing the dataset.\n",
    "    \"\"\"\n",
    "    available: List[str] = [folder for folder in path_to_dataset.iterdir() if folder.name.startswith(\"my\")]\n",
    "\n",
    "    if not any(name == x.name for x in available):\n",
    "        Exception(\"Dataset not found\")\n",
    "\n",
    "    ret: Dict = {}\n",
    "\n",
    "    for folder in (path_to_dataset / name).iterdir():\n",
    "        try:\n",
    "            cur: Dict = {}\n",
    "            cur[\"name\"] = folder.name\n",
    "            cur[\"gt\"] = []\n",
    "\n",
    "            folder_path: Path = path_to_dataset / name / folder\n",
    "\n",
    "            with open(folder_path / \"groundtruth.txt\") as f:\n",
    "                for line in f:\n",
    "                    cur[\"gt\"].append([int(float(x)) for x in line.split(\",\")])\n",
    "\n",
    "            cur[\"image_files\"] = [\n",
    "                str(folder_path / \"img\" / x.name) for x in (folder_path / \"img\").iterdir()\n",
    "            ]\n",
    "\n",
    "            if len(cur[\"gt\"]) != len(cur[\"image_files\"]):\n",
    "                print(f\"Error while loading dataset {folder} gt and image files have different length\")\n",
    "                continue\n",
    "\n",
    "            ret[folder.name] = cur\n",
    "        except:\n",
    "            print(\"Error while loading dataset\", folder)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a dataset.py\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load all the datasets.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing all the datasets.\n",
    "    \"\"\"\n",
    "    ret: Dict = {}\n",
    "    for folder in path_to_dataset.iterdir():\n",
    "        if folder.name.startswith(\"my\"):\n",
    "            ret[folder.name] = load_dataset(folder.name)\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948649c324ca7862321fb27243ea9939e0d68076f558125e86352d86ba9fdaa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
